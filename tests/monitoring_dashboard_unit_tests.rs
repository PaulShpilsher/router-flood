//! Unit tests for monitoring dashboard module\n//!\n//! These tests were moved from src/monitoring/dashboard.rs to maintain\n//! separation between implementation and test code.\n\nuse router_flood::monitoring::dashboard::*;\nuse router_flood::monitoring::metrics::MetricsCollector;\nuse std::sync::Arc;\nuse std::time::Duration;\n\n#[test]\nfn test_dashboard_creation() {\n    let collector = Arc::new(MetricsCollector::new());\n    let dashboard = PerformanceDashboard::new(collector, Duration::from_secs(1));\n    \n    let state = dashboard.get_dashboard_state();\n    assert!(state.uptime_seconds >= 0.0);\n    assert_eq!(state.performance_indicators.success_rate_percent, 100.0);\n}\n\n#[test]\nfn test_performance_indicators() {\n    let collector = Arc::new(MetricsCollector::new());\n    let dashboard = PerformanceDashboard::new(collector, Duration::from_secs(1));\n    \n    // Simulate some metrics\n    dashboard.router_metrics.packets_sent.update(1000.0, std::collections::HashMap::new());\n    dashboard.router_metrics.packets_failed.update(10.0, std::collections::HashMap::new());\n    \n    let indicators = dashboard.calculate_performance_indicators();\n    assert!((indicators.success_rate_percent - 99.01).abs() < 0.1);\n}\n\n#[test]\nfn test_alerts() {\n    let collector = Arc::new(MetricsCollector::new());\n    let dashboard = PerformanceDashboard::new(collector, Duration::from_secs(1));\n    \n    // Set high CPU usage to trigger alert\n    dashboard.router_metrics.cpu_usage.update(95.0, std::collections::HashMap::new());\n    \n    let alerts = dashboard.check_alerts();\n    assert!(!alerts.is_empty());\n    assert!(matches!(alerts[0].level, AlertLevel::Critical));\n}\n"